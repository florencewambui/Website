<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Text Classification with Naive Bayes">
  <meta name="generator" content="Hugo 0.62.1" />

  <title>Flagging Toxic Comments Part 1 &middot; Florence Muriuki</title>

    

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="/css/blackburn.css">

  
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.9.0/css/all.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

 
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/androidstudio.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  
  <script>hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="/">Wambui</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/contact/"><i class='fa fa-phone fa-fw'></i>Contact</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/https://www.linkedin.com/in/florence-muriuki-57239a143/" rel="me" target="_blank"><i class="fab fa-linkedin"></i></i>LinkedIn</a>
    </li>
    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/florencewambui" rel="me" target="_blank"><i class="fab fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2016. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Flagging Toxic Comments Part 1</h1>
  <h2>Text Classification with Naive Bayes</h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>14 Jan 2020, 00:00</time>
  </div>

  

  
  
  
  

  
  
  
  

</div>

  


<div id="summary" class="section level4">
<h4>Summary</h4>
<p><em>In this post, we cover descriptions of Bayes Theorem and Naive Bayes. We then
use Naive Bayes to classify Wikipedia comments as harmful or harmless. The model created detects 58% of harmful comments in the test data. In future posts, we improve the model by using more features and different classification models.</em></p>
</div>
<div id="introduction" class="section level4">
<h4>Introduction</h4>
<p>Most of us find the internet entertaining and resourceful, but sometimes we come across perjoratives in various forums. Text classification can automate the identification of harmful comments hence preventing them from been posted or deleting them as soon as they are posted, making the internet safe and friendly for all of us. In this post, we train a Naive Bayes classifier to identify harmful comments. The data is from <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">this Kaggle Toxic Comment Classification Challenge</a>.</p>
<p>Let’s get started.</p>
</div>
<div id="explore-the-data" class="section level4">
<h4>Explore the data</h4>
<p>Training data, as provided by Kaggle, has 159571 rows and 8 variables. The variables are as follows:</p>
<pre><code>* id - unique identifier of each comment
* comment_text - the actual comments from wikipedia
* toxic - binary column taking 1 for toxic comments and 0 otherwise
* severe_toxic - binary column taking 1 for severe toxic comments and 0 otherwise
* obscene -binary column taking 1 for obscene comments and 0 otherwise
* threat - binary column taking 1 for threat comments and 0 otherwise
* insult - binary column taking 1 for insult comments and 0 otherwise
* identity_hate - binary column taking 1 for identity hate comments and 0 otherwise</code></pre>
<p>The Kaggle training data looks as shown below:
<img src="/./toxic_comment_blog_part1_files/train_head.PNG" alt="glimpse of data" />. The Kaggle testing data contains only the id and comment_text columns.</p>
<p>The Kaggle Challenge is a multilabel classification (a comment is assigned more than one toxicity level - toxic, severe toxic etc), but in this post we will focus on just whether a comment is harmful or not. We create a column that indicates whether a column is harmful (takes value 1 in any of the 5 levels of toxicity) as shown below:</p>
<p><img src="/post/toxic_comment_blog_part1_files/harmful%20column.PNG" /></p>
<p>The percentage of harmful comment is only 10% making this a highly imbalanced dataset.
<img src="/post/toxic_comment_blog_part1_files/proportion%20harmful.PNG" />.</p>
<p>We then split the data into two to create our own training and testing datasets.
<img src="/post/toxic_comment_blog_part1_files/split.PNG" alt="splitting data" /></p>
</div>
<div id="building-a-simple-classification-model-with-naive-bayes" class="section level4">
<h4>Building a simple classification model with Naive Bayes</h4>
<p>Going by the fact that words used are the most obvious indicator of whether a comment is harmless or harmful, we will build a simple Naive Bayes model that will classify comments as harmful or otherwise. Naive Bayes is one of my favourite machine learning algorithms as I find it simple and intuitive.</p>
<div id="my-understanding-of-naive-bayes" class="section level5">
<h5>My understanding of Naive Bayes</h5>
<p>Say you wake up in the morning and are deciding on something to wear. For simplicity, let us assume you only have 5 sets of clothes, some lighter than others, but you like them equally and the weather has been alternating between cold and warm days. At first, your curtains are drawn closed so you have no idea whether its cloudy or sunny. The probability of picking any of set of clothes is 0.2 (1/5) as you have 5 sets and you like all pairs equally. You decide to draw the curtain open, and voila, it’s sunny. Suppose that of your 5 pairs of clothes, 3 pairs are lighter and hence more comfortable for warm days.Given how sunny it looks, you will shift your attention from all the 5 sets of clothes to the 3 sets of light clothes. Now, the probability of picking any of the 3 sets of clothes is 0.33 (1/3) and the probability of picking any of the other 2 sets of clothes is 0.</p>
</div>
<div id="mathematics-of-naive-bayes" class="section level5">
<h5>Mathematics of Naive Bayes</h5>
<p>Naive Bayes is an application of Bayes theorem that states:</p>
<pre><code>       P(H|A) = P(A|H)P(A)/P(H)</code></pre>
<p>Let’s take a different example. Suppose you have 5 wikipedia comments and you know that 2 of them are harmful meaning that the probability that any comment picked at random, without looking at its wording, is harmful is 0.2 (1/5). Let’s call this P(H).</p>
<p>Looking at the comments, you realize that 3 of them contain the word “ass” hence the probability that any randomly picked comment contains the word “ass” is 0.33 (1/3). Let’s call this P(A).</p>
<p>Further looking at the comments reveals that of the 2 harmful comments, only 1 contains the word “ass” so the probability that a comment contains the word ass when we know that the comment is harmful is 0.5 (1/2). We call this P(A|H).</p>
<p>Let’s define P(H|A) as the probability that a comment is harmful when we know that it contains the word “ass” such that we calculate P(H|A) as 0.5 * 0.2/0.33 = 0.3. Now we know that if we pick any comment and glimpse only the word “ass” in it, it is 30% likely to be a harmful comment. 30% is higher than our original estimate of 20% when we picked a comment and did not have a look at any of its wording. The knowledge that the comment contains the word “ass” makes us know that it is more likely to be a harmful comment.</p>
<p>Naive Bayes applies Bayes Theorem, but with a few tweaks to make it more robust. In this case, Naive Bayes will find evidence of a comment been harmful or harmless based on the words used. To learn more about Naive Bayes, please read Chapter 4 of Machine Learning with R by Brett Lantz which offers a simple and comprehensive explanation.</p>
</div>
</div>
<div id="implementation-of-naives-bayes-using-pythons-scikit-learn" class="section level4">
<h4>Implementation of Naives Bayes using Python’s Scikit Learn</h4>
<div id="step-1-clean-the-comments" class="section level5">
<h5>Step 1: Clean the comments</h5>
<p>We only want to use words for classification thus we need to clean the comments to ensure that we remain mostly with meaningful words.We start by cleaning/uniformizing the comments by changing the comments to lower case and removing characters that are not letters.</p>
<p><img src="/post/toxic_comment_blog_part1_files/cleaning%20comments.png" /></p>
<p>Then we remove english stopwords, words with less than 3 characters and we lemmatize each word in the comments.</p>
<p><img src="/post/toxic_comment_blog_part1_files/cleaning%20comments%202.PNG" /></p>
</div>
<div id="step-2-transform-the-comments-into-a-vector" class="section level5">
<h5>Step 2: Transform the comments into a vector</h5>
<p>As earlier mentioned, we will use words as the predictors of harmful comments in our model thus we need to transform the comments into a vector such that each comment is a row and each unique word is a column. Two of the commonly used vectors in Sklearn are:</p>
<ul>
<li>Count Vectors implemented with CountVectorizer()</li>
</ul>
<p>A count vector is a matrix in which each coment is a row, each column
is a unique word from the collection of sentences and each cell value is the
number of times a particular word appears in a comment.</p>
<p>Say you have the following sentences:</p>
<ol style="list-style-type: decimal">
<li>the cat is black and white</li>
<li>the dog is white and brown</li>
</ol>
<p>Then the count vector looks as shown below:</p>
<p><img src="/post/toxic_comment_blog_part1_files/countVectorizer.PNG" /></p>
<ul>
<li>TFIDF Vectors implemented with TfidfVectorizer()</li>
</ul>
<p>Depending on the context, you may have words that appear in almost all comments such as
determiners, pronouns, prepositions and contextual vocabularies. These common words can biase your dataset (make it seem like they have greater predictive power than they actually do). This is where IDF comes in to balance things out by assigning less weight to frequently occurring words and more weight to rarely occurring words.</p>
<p>TF represents term frequency - the number of times a word appears in a comment(what we got in count vector above). Sometimes term frequency is normalized by calculating it as:</p>
<pre><code>        = Number of times a word appears in a comment/Total number of words in a comment
             </code></pre>
<p>IDF represents Inverse Document Frequency and is calculated as:</p>
<pre><code>        = log(Number of comments/Number of comments in which a particular word appears)</code></pre>
<p>From this formula you see that a greater denominator will result into a smaller figure. From our sentences above, we calculate the IDF of “black” as:</p>
<pre><code>            = log(2/1) as there are 2 sentences and &quot;black&quot; appears in 1 sentence.</code></pre>
<p>TFIDF is the product of TF and IDF. Sklearn documentation offers a very good explanation of count and tfidf vectors <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">here</a>. We will use TFIDF vectors:</p>
<p><img src="/post/toxic_comment_blog_part1_files/tfidf.PNG" /></p>
</div>
<div id="step-3-build-the-model" class="section level5">
<h5>Step 3: Build the model</h5>
<p><img src="/post/toxic_comment_blog_part1_files/naive%20bayes.PNG" /></p>
</div>
<div id="step-4-predict-whether-comments-in-our-test-data-are-harmful-or-not" class="section level5">
<h5>Step 4: Predict whether comments in our test data are harmful or not</h5>
<p>We first transform the comments in the test data as we transformed those in the training data then apply our classification model onto the test data comments and evaluate how well our model does.</p>
<p><img src="/post/toxic_comment_blog_part1_files/predict.PNG" /></p>
<p>As the data is highly imbalanced, accuracy is a poor metric because if you guess that a comment is harmless, 90% of the time you will be correct. We will therefore use precision, recall and F1 score as our evaluation metrics. As a reminder, earlier on we assigned 1 to comments that are harmful thus harmful is our positive class while harmless is the negative class.</p>
<p>Precision is the proportion of comments classified by our model as harmful that are actually harmful. Precision is calculated as:</p>
<pre><code>            TP/(TP + FP) where TP (True Positive) means the number of comments in the positive (harmful) class that are predicted by the model to be positive. 
            
                               FP (False Postive) means the number of comments in the negative (harmless) class that are predicted by the model to be positive.</code></pre>
<p>Our model classifies 92% of harmful comments as harmful meaning that of all the comments that were predicted as harmful, 8% were actually harmless.</p>
<p>Recall is the proportion of harmful comments that were classified by our model as harmful and is calculated as:</p>
<pre><code>        TP/(TP + FN) where FN (False negative) means the number of comments that are in the positive (harmful) class but are predicted by the model to be negative.
        </code></pre>
<p>A recall of 54% means that the model detected only 54% of harmful comments and incorrectly classified 46% harmful comments as harmless. We want to flag off as many harmful comments as possible thus recall is the most important metric and we need it to be way higher.</p>
<p>F1 score is a composite of precision and recall therefore we also need it to increase as we increase the recall.</p>
</div>
<div id="step-5-improve-the-model-by-trying-different-parameters" class="section level5">
<h5>Step 5: Improve the model by trying different parameters</h5>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer()</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">MultinomialNB()</a> take a couple of parameters that can help with fine tuning our model (visit the links to see explanation of all parameters). In this example, we use alpha from MultinomialNB(), min_df, sublinear_tf, ngram_range, use_idf and smooth idf from TfidfVectorizer() to improve our recall score.</p>
<p><img src="/post/toxic_comment_blog_part1_files/grid_search.PNG" /></p>
<p><img src="/post/toxic_comment_blog_part1_files/grid_search_results.PNG" /></p>
<p>Using the best parameters from the grid search results, we create our final Naive Bayes model as:
<img src="/post/toxic_comment_blog_part1_files/final%20naive%20bayes.PNG" /></p>
<p>The test data recall score improved by only 4% reaching 58%. In the next post, we will try to improve this score by using feature engineering and Logistic Regression.</p>
<p>Data source: <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data" class="uri">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data</a></p>
<p>References:</p>
<ul>
<li><p>Machine Learning with R by Brett Lantz Chapter 4: Probabilistic Learning – Classification
Using Naive Bayes</p></li>
<li><p>Building Machine Learning Systems with Python by Willi Richerto and Luis Pedro Cooelho Chapter 6</p></li>
<li><p><a href="https://towardsdatascience.com/naive-bayes-intuition-and-implementation-ac328f9c9718" class="uri">https://towardsdatascience.com/naive-bayes-intuition-and-implementation-ac328f9c9718</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html</a></p></li>
</ul>
<p>Tools:</p>
<ul>
<li>R</li>
<li>Python</li>
</ul>
<p><a href="https://github.com/florencewambui/Website/tree/master/content/post/Kaggle-Toxic-Comment-Challenge/Scripts">Code repository</a></p>
<p>#rstats #naivebayes #nlp #flask</p>
</div>
</div>

  
  <h4><i class="fa-share-alt" aria-hidden="true"></i>&nbsp;Share!</h4>
<ul class="share-buttons">
	<li><a href="https://www.facebook.com/sharer/sharer.php?u=%2fpost%2ftoxic_comment_blog_part1%2f" target="_blank" title="Share on Facebook"><i class="fa-facebook" aria-hidden="true"></i><span class="sr-only">Share on Facebook</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://twitter.com/intent/tweet?source=%2fpost%2ftoxic_comment_blog_part1%2f&via=HorribleGeek" target="_blank" title="Tweet"><i class="fa-twitter" aria-hidden="true"></i><span class="sr-only">Tweet</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://plus.google.com/share?url=%2fpost%2ftoxic_comment_blog_part1%2f" target="_blank" title="Share on Google+"><i class="fa-google-plus" aria-hidden="true"></i><span class="sr-only">Share on Google+</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.tumblr.com/share?v=3&u=%2fpost%2ftoxic_comment_blog_part1%2f" target="_blank" title="Post to Tumblr"><i class="fa-tumblr" aria-hidden="true"></i><span class="sr-only">Post to Tumblr</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://pinterest.com/pin/create/button/?url=%2fpost%2ftoxic_comment_blog_part1%2f" target="_blank" title="Pin it"><i class="fa-pinterest-p" aria-hidden="true"></i><span class="sr-only">Pin it</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.reddit.com/submit?url=%2fpost%2ftoxic_comment_blog_part1%2f" target="_blank" title="Submit to Reddit"><i class="fa-reddit-alien" aria-hidden="true"></i><span class="sr-only">Submit to Reddit</span></a>
	</li>
</ul>


<style>
	ul.share-buttons{
	  list-style: none;
	  padding: 0;
	}

	ul.share-buttons li{
	  display: inline;
	}

	ul.share-buttons .sr-only{
	  position: absolute;
	  clip: rect(1px 1px 1px 1px);
	  clip: rect(1px, 1px, 1px, 1px);
	  padding: 0;
	  border: 0;
	  height: 1px;
	  width: 1px;
	  overflow: hidden;
	}
</style>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="/post/kenya_newspaper_headlines_blog/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="/post/kenya_newspaper_headlines_blog/">Past 10 years in headlines: Uhuru&#39;s Friends and Enemies, Corruption Web</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="/post/toxic_comment_blog_part2/">Flagging Toxic Comments Part 2</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="/post/toxic_comment_blog_part2/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  

</div>

</div>
</div>
<script src="/js/ui.js"></script>
<script src="/js/menus.js"></script>








</body>
</html>

