{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"toxicity_score\"] = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis =1)\n",
    "train[\"harmful\"] = np.where(train[\"toxicity_score\"] > 0, 1, 0)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"harmful\"].value_counts())\n",
    "round(len(train[train[\"harmful\"] == 1])/len(train) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[\"comment_text\"], train[\"harmful\"], \n",
    "                                                    test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text - remove everything but words and spaces, remove extra spaces, \n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    pat = re.compile(r\"[^A-Za-z\\s']\")\n",
    "    text = pat.sub(\" \", text)\n",
    "    text = text.rstrip()\n",
    "    newLines = re.compile(r\"[\\n\\r\\t]\")\n",
    "    text = newLines.sub(\" \", text)\n",
    "    extraspace = re.compile(r'\\s{2,}')\n",
    "    text = extraspace.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "X_train = X_train.map(clean_text)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize, stem, remove stopwords, remove words with less than 3 characters\n",
    "eng_stopwords = [set(stopwords.words('english')), \"i'm\", \"can't\", \"you\"]\n",
    "def preprocess_text(text): \n",
    "    text = \" \".join([word for word in text.split() if len(word) >2])\n",
    "    text = \" \".join([word for word in text.split() if word not in eng_stopwords])\n",
    "    text = \" \".join([WordNetLemmatizer().lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "X_train = X_train.map(preprocess_text)\n",
    "X_train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectClean = TfidfVectorizer(min_df = 50, strip_accents = \"unicode\").fit(X_train)\n",
    "X_train_dtm_tfidf = tfidfVectClean.transform(X_train)\n",
    "\n",
    "nbModel = MultinomialNB(alpha = 0.1)      \n",
    "nbModel.fit(X_train_dtm_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.map(clean_text)\n",
    "X_test = X_test.map(preprocess_text)\n",
    "predictions = nbModel.predict(tfidfVectClean.transform(X_test))\n",
    "print(\"Accuracy score: \", round(accuracy_score(y_test, predictions),3))     \n",
    "print(\"Precision score: \", round(precision_score(y_test, predictions),3))    \n",
    "print(\"Recall score: \", round(recall_score(y_test, predictions), 3)) \n",
    "print(\"F1 score: \", round(f1_score(y_test, predictions), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {'clf__alpha': (0.1, 0.5, 1), \n",
    "              'tfidf__min_df': (50,100,500,1000),\n",
    "              'tfidf__sublinear_tf': (True, False),\n",
    "              'tfidf__ngram_range': ((1,1), (1,2), (1,3)),\n",
    "              'tfidf__use_idf':(True, False),\n",
    "              'tfidf__smooth_idf': (True, False)\n",
    "             }\n",
    "grid_search = GridSearchCV(pipeline, parameters, scoring = \"recall\")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectClean = TfidfVectorizer(strip_accents = \"unicode\", min_df = 50, ngram_range = (1,3), smooth_idf = False, sublinear_tf = True, use_idf = True).fit(X_train)\n",
    "X_train_dtm_tfidf = tfidfVectClean.transform(X_train)\n",
    "nbModel = MultinomialNB(alpha = 0.1)      \n",
    "nbModel.fit(X_train_dtm_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nbModel.predict(tfidfVectClean.transform(X_test))\n",
    "print(\"Accuracy score: \", round(accuracy_score(y_test, predictions),3))     \n",
    "print(\"Precision score: \", round(precision_score(y_test, predictions),3))    \n",
    "print(\"Recall score: \", round(recall_score(y_test, predictions), 3))        \n",
    "print(\"F1 score: \", round(f1_score(y_test, predictions), 3))                 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
